{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problems\n",
    "\n",
    "* vanishing/exploding gradients\n",
    "* training would be extemely slow\n",
    "* too many parameters would risk overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vanishing/Exploding gradients\n",
    "\n",
    "The backpropagation works by going from the output layer to the input layer. It use the gradient of the cost function with regards to each parameter in the network.\n",
    "Gradients often gets smaller as the algorithm progresses down to the lower layer. The lower layer connection weights virtually unchanged and training never converges to a good solution. \n",
    "In some cases, opposite can happen: the gradients can grow bigger and bigger, so many layers get insanely large weight updates and the algorithm diverges. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## reason\n",
    "With the old activation function and initialization, the variance of the outputs of each layer is much greater than the variance of its input. For example, for logistic activation function(sigmoid function), when inputs become large the function saturates at 0 or 1, with a derivative extremely close to 0. There is no gradient to propagate back through the network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialization \n",
    "<img src=\"Initialization.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nonsaturating activation functions\n",
    "* Leak ReLU:\n",
    "LReLU(z) = max($\\alpha$z,z) , typically $\\alpha$ set to 0.01(small leak)\n",
    "\n",
    "* Randomized leaky ReLU(RReLU): $\\alpha$ is picked randomly in a given range during training and it  is fixed to an average value during testing (act as a regularizer which reduce the risk of overfitting)\n",
    "\n",
    "* Parametric leaky ReLU(PReLU): $\\alpha$ is authorized to be learned during training (instead of being a hyperparameter, it can be modified by backpropagation) <font color='red'>ourperform  on  large image datasets, but on smaller datasets it runs the risk of overfitting the training set</font>\n",
    "\n",
    "* Exponential linear unit(ELU):\n",
    "$$ ELU_{\\alpha}=\\left\\{\n",
    "\\begin{aligned}\n",
    "&\\alpha(exp(z)-1)  & \\text{if } z<0 \\\\\n",
    "&z & \\text{if } z \\geq 0\\\\\n",
    "\\end{aligned}\n",
    "\\right.\n",
    "$$\n",
    "$\\alpha$ is usually set to 1.\n",
    "\n",
    "<img src='ELU.png' />\n",
    "\n",
    "### which activation?\n",
    "In general: ELU>Leaky ReLU>ReLU>tanh>logistic\n",
    "If you care more about runtime, then you may prefer leaky ReLUs over ELUs. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch Normalization\n",
    "Internal Covariate Shift problem: the distribution of each layer's inputs changes during training, as the parameters of the previous layers change.\n",
    "#### Solution: \n",
    "add an operation in the model just before the activation function of each layer, simply zero-centering and normalizing the inputs, then scaling and shifting.\n",
    "At test time, use the whole training set's mean and standard deviation.\n",
    "\n",
    "The vanishing gradients problems was strongly reduced, to the point that they could use saturaing activation functions like logistic and tanh function. The network were also less sensitive to the weight initialization.\n",
    "\n",
    "Batch normalization add some complexity to the model. If you need predictions to be lightning-fast, you may want to check how well plain ELU+ He initialization perform before playing with batch normalization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Clipping\n",
    "A technique to lessen the exploding gradients problem is to clip the gradients during backpropagation so that they never exceed some threshold.\n",
    "\n",
    "In TensorFlow, the optimizer's minimize() function takes care of both computing the gradients and applying them. So you first call <code> compute_gradients()</code> first, then create an operation of clip the gradients using <code>clip_by_value()</code>, and finally create an operation to apply the clipped gradients using optimizer's <code>apply_gradients()</code> method.\n",
    "```\n",
    "threshold = 1.0\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "grads_and_vars = optimizer.computer_gradients(loss)\n",
    "capped_gvs = [(tf.clip_by_value(grad, -threshold, threshold), var) for grad, var in grads_and_vars]\n",
    "training_op = optimizer.apply_gradients(capped_gvs)\n",
    "```\n",
    "The gradients will be clipped between -1.0 and 1.0 and applied. The threshold is a hyperparameter you can tune."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reusing Pretrained Layers\n",
    "It is generally not a good idea to train a very large DNN from scratch. You should try to find an existing neural network that accomplishes a similar task and just resuse the lower layers of this network. It will not only speed up training considerably, but will also require much less training data.\n",
    "<img src = 'Reuse_layers.png'/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.contrib.layers import fully_connected "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "x  = tf.placeholder(tf.float32,shape=(None,10),name='x')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden1 = fully_connected(x,10,scope='hidden1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tfkernel",
   "language": "python",
   "name": "tfkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
